# -*- coding: utf-8 -*-
"""Langchain-RAG-App-Semantic-chunks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kl1VP5_7avH41eaQYkZSzP9c69-A4YAr
"""

# !pip install langchain
# !pip install langchain_community
# !pip install langchain_experimental
# !pip install langchain_openai
# !pip install faiss-cpu
# !pip install langchainhub

# !wget https://gutenberg.org/cache/epub/14586/pg14586.txt -O the_brain.txt

with open("./the_brain.txt") as f:
  the_brain = f.read()

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=0,
    length_function=len
)

naive_chunks = text_splitter.split_text(the_brain)

for chunk in naive_chunks[40:55]:
  print(chunk + "\n")

import os
# os.environ['OPENAI_API_KEY'] = 'xxx'

"""# Chunk semantico"""

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

semantic_chunker = SemanticChunker(OpenAIEmbeddings(model="text-embedding-3-large"), breakpoint_threshold_type="percentile")

semantic_chunks = semantic_chunker.create_documents([the_brain])

for semantic_chunk in semantic_chunks:
  if "MDT is associated with the basic" in semantic_chunk.page_content:
    print(semantic_chunk.page_content)
    print(len(semantic_chunk.page_content))

"""# RAG App
Usaremos LCEL y FAISS (facebook AI Similarity Search) la cual es una biblioteca optimizada para realizar busquedas rapidas en grandes bases de datos de vectores almacenados en memoria, ideal para tareas de recuperacion de informacion y sistemas de recomendacion debido a su alta velocidad y escalabilidad.
"""

from langchain_community.vectorstores import FAISS

semantic_chunk_vectorstore = FAISS.from_documents(semantic_chunks, embedding=OpenAIEmbeddings(model="text-embedding-3-large"))

semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={"k": 1})

semantic_chunk_retriever.invoke("what is MDT?")

from langchain_core.prompts import ChatPromptTemplate

rag_template = """\
Use the following context to anser the user's query. If you cannot answer, please respond with 'I don't know'.

User's Query:
{questions}

Context:
{context}
"""

from langchain import hub

# os.environ["LANGCHAIN_API_KEY"] = "xxxxx"
prompt = hub.pull("rlm/rag-prompt")

"""# Generation"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# LCEL (LanghChain Expression Language) RAG (Retrieval-Augmented Generation) Chain

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

semantic_rag_chain = (
    {"context": semantic_chunk_retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


question = "what is MDT?"
print("question:", question)
print(semantic_rag_chain.invoke(question))

# # semantic_rag_chain.invoke("que es MDT? traducelo al ingles y luego traduce la respuesta al espa√±ol")
# semantic_rag_chain.invoke("que es MDT?")
# # semantic_rag_chain.invoke("que significa MDT?")

# semantic_rag_chain.invoke("que es un ZM?")

# semantic_rag_chain.invoke("haz un resumen completo de todo el texto")

# semantic_rag_chain.invoke("Que es la despersonalizacion?")

